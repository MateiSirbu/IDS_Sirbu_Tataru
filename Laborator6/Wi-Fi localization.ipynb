{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wi-Fi localization. Modele de clasificare\n",
    "\n",
    "Sîrbu Matei-Dan, _grupa 10LF383_\n",
    "\n",
    "<i>Sursă dataset:</i> http://archive.ics.uci.edu/ml/datasets/Wireless+Indoor+Localization\n",
    "\n",
    "<i>Descriere dataset:</i> [DOI 10.1007/978-981-10-3322-3_27 via ResearchGate](Docs/chp_10.1007_978-981-10-3322-3_27.pdf)\n",
    "\n",
    "<i>Synopsis:</i> Setul de date _Wireless Indoor Localization_ cuprinde 2000 de măsurători ale puterii semnalului (măsurat în dBm) recepționat de la routerele unui birou din Pittsburgh. Acest birou are șapte routere și patru camere; un utilizator înregistrează cu ajutorul unui smartphone o dată pe secundă puterea semnalelor venite de la cele șapte routere, fiecărei înregistrări fiindu-i asociate camera în care se afla utilizatorul la momentul măsurării (1, 2, 3 sau 4).\n",
    "\n",
    "În figura de mai jos este ilustrat un sample din dataset: <br><br>\n",
    "![Sample](./Images/wifi_localization_sample.png)\n",
    "\n",
    "În cele ce urmează, coloana Class (camera) este reprezentată de y, iar coloanele WS1 - WS7 (features: puterea semnalului de la fiecare router), de X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Dataset overview:</i>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WS1</th>\n",
       "      <th>WS2</th>\n",
       "      <th>WS3</th>\n",
       "      <th>WS4</th>\n",
       "      <th>WS5</th>\n",
       "      <th>WS6</th>\n",
       "      <th>WS7</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-64</td>\n",
       "      <td>-56</td>\n",
       "      <td>-61</td>\n",
       "      <td>-66</td>\n",
       "      <td>-71</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-68</td>\n",
       "      <td>-57</td>\n",
       "      <td>-61</td>\n",
       "      <td>-65</td>\n",
       "      <td>-71</td>\n",
       "      <td>-85</td>\n",
       "      <td>-85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-63</td>\n",
       "      <td>-60</td>\n",
       "      <td>-60</td>\n",
       "      <td>-67</td>\n",
       "      <td>-76</td>\n",
       "      <td>-85</td>\n",
       "      <td>-84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-61</td>\n",
       "      <td>-60</td>\n",
       "      <td>-68</td>\n",
       "      <td>-62</td>\n",
       "      <td>-77</td>\n",
       "      <td>-90</td>\n",
       "      <td>-80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-63</td>\n",
       "      <td>-65</td>\n",
       "      <td>-60</td>\n",
       "      <td>-63</td>\n",
       "      <td>-77</td>\n",
       "      <td>-81</td>\n",
       "      <td>-87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-59</td>\n",
       "      <td>-59</td>\n",
       "      <td>-48</td>\n",
       "      <td>-66</td>\n",
       "      <td>-50</td>\n",
       "      <td>-86</td>\n",
       "      <td>-94</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-59</td>\n",
       "      <td>-56</td>\n",
       "      <td>-50</td>\n",
       "      <td>-62</td>\n",
       "      <td>-47</td>\n",
       "      <td>-87</td>\n",
       "      <td>-90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-62</td>\n",
       "      <td>-59</td>\n",
       "      <td>-46</td>\n",
       "      <td>-65</td>\n",
       "      <td>-45</td>\n",
       "      <td>-87</td>\n",
       "      <td>-88</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-62</td>\n",
       "      <td>-58</td>\n",
       "      <td>-52</td>\n",
       "      <td>-61</td>\n",
       "      <td>-41</td>\n",
       "      <td>-90</td>\n",
       "      <td>-85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-59</td>\n",
       "      <td>-50</td>\n",
       "      <td>-45</td>\n",
       "      <td>-60</td>\n",
       "      <td>-45</td>\n",
       "      <td>-88</td>\n",
       "      <td>-87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WS1  WS2  WS3  WS4  WS5  WS6  WS7  Class\n",
       "0     -64  -56  -61  -66  -71  -82  -81      1\n",
       "1     -68  -57  -61  -65  -71  -85  -85      1\n",
       "2     -63  -60  -60  -67  -76  -85  -84      1\n",
       "3     -61  -60  -68  -62  -77  -90  -80      1\n",
       "4     -63  -65  -60  -63  -77  -81  -87      1\n",
       "...   ...  ...  ...  ...  ...  ...  ...    ...\n",
       "1995  -59  -59  -48  -66  -50  -86  -94      4\n",
       "1996  -59  -56  -50  -62  -47  -87  -90      4\n",
       "1997  -62  -59  -46  -65  -45  -87  -88      4\n",
       "1998  -62  -58  -52  -61  -41  -90  -85      4\n",
       "1999  -59  -50  -45  -60  -45  -88  -87      4\n",
       "\n",
       "[2000 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "header = ['WS1', 'WS2', 'WS3', 'WS4', 'WS5', 'WS6', 'WS7', 'Class']\n",
    "data_wifi = pd.read_csv(\"./Datasets/wifi_localization.txt\", names=header, sep='\\t')\n",
    "display(HTML(\"<i>Dataset overview:</i>\"))\n",
    "display(data_wifi)\n",
    "X = data_wifi.values[:, :7]\n",
    "y = data_wifi.values[:, -1]\n",
    "folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Modelele de clasificare _in a nutshell_</u>\n",
    "Pentru clasificarea datelor din datasetul _Wi-Fi localization_ vom utiliza următoarele modele de clasificare: kNN, Decision Tree, MLP, Gaussian NB și Random Forest. În paragraful următor vă vom explica modul de funcționare al fiecărui algoritm și evidenția hiperparametrii și formulele care stau la baza acestora.\n",
    "<div style=\"text-align:center\"><img src=\"./Images/xkcd_machine_learning.png\"><br>\"hiperparametrii și formulele care stau la baza acestora\"<br>sursă: <a href=\"https://xkcd.com/1838/\">xkcd 1838: Machine Learning</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1. <i>k</i>-nearest neighbors classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.neighbors.KNeighborsClassifier`<i>(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Într-o problemă de clasificare, algoritmul kNN (_k_-nearest neighbors) identifică cei mai apropiați _k_ vecini ai fiecărui item neclasificat - indiferent de etichetele acestora - vecini localizați în setul de antrenare. Determinarea claselor din care fac parte itemii neclasificați se face prin votare: clasa în care aparțin majoritatea vecinilor se consideră clasa itemului.\n",
    "<div style=\"text-align:center\"><img style=\"width: 500px\" src=\"./Images/knn_example.png\"><br>Exemplu: clasificarea itemului c cu 3NN. În urma votării se determină clasa lui c: <b>o</b>.<br>sursă: <a href=\"http://youtu.be/UqYde-LULfs\">YouTube (<i>How kNN algorithm works</i> de Thales Sehn Körting)</a></div><br>\n",
    "\n",
    "Pentru determinarea distanței dintre itemi se pot utiliza mai multe metrici. Scikit-learn admite orice funcție Python ca și metrică, dar implicit folosește metrica _Minkowski_. Iată câteva exemple de metrici des utilizate în kNN:\n",
    "\n",
    "- _distanța Minkowski_: $d_{st} = \\sqrt[p]{\\sum_{j=1}^n |x_{sj} - y_{tj}|^p}$  (_Obs._: p este un hiperparametru utilizat de Scikit-learn)\n",
    "- _distanța Euclideană_: $d(\\textbf{x},\\textbf{y}) = \\sqrt{\\sum_{i=1}^n (y_i - x_i)^2}$\n",
    "- _distanța Manhattan (City block)_: $d_{st} = \\sum_{j=1}^n |x_{sj} - y_{tj}|$\n",
    "- _distanța Mahalanobis_: $d(\\textbf{x},\\textbf{y}) = \\sqrt{\\sum_{i=1}^n \\frac{(x_i - y_i)^2}{s_i^2}}$, unde $s_i$ este deviația standard a lui $x_i$ și $y_i$ în sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2. Decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.tree.DecisionTreeClassifier`<i>(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un arbore de decizie (_decision tree_) este o structură arborescentă tip flowchart unde un nod intern reprezintă un feature, ramura este un criteriu de decizie, iar fiecare frunză este un rezultat, o clasificare. Algoritmul Decision tree selectează cel mai bun feature folosind o metrică ASM (_Attribute Selection Measure_), convertește un nod feature la un nod tip criteriu de decizie, și partiționează (splits) datasetul în subseturi. Procesul se execută recursiv până arborele conține numai noduri criterii de decizie și noduri frunză rezultat. Cu cât arborele este mai adânc, cu atât sunt mai complexe criteriile de decizie și modelul are o acuratețe mai mare. \n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width: 600px\" src=\"./Images/dt_diagram.png\"><br>Structura unui arbore de decizie.<br>sursă: <a href=\"https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html\">KDnuggets (Decision Tree Algorithm, Explained)</a></div>\n",
    "\n",
    "<br>Pentru măsurarea calității unui split, Scikit-learn utilizează două metrici ASM:\n",
    "\n",
    "- _impuritatea Gini_ (cât de des este etichetat greșit un element ales aleator dacă a fost etichetat folosind distribuția etichetelor dintr-un subset; poate determina overfitting-ul modelului): <br>$Gini(p) = 1 - \\sum_{j=1}^c p_j^2$ <br>\n",
    "- _entropia_ (similar cu Gini impurity, mai intensă d.p.d.v. computațional din cauza funcției logaritmice): <br>$H(p) = - \\sum_{j=1}^c p_j \\log p_j$\n",
    "\n",
    "(unde c este numărul de clase (etichete), iar $p_j$ este subsetul etichetat cu clasă i, unde $j \\in \\{1, 2, ..., c\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3. Multilayer perceptron (MLP) classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.neural_network.MLPClassifier`<i>(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Perceptronii_ sunt o clasă de clasificatori utilizați în învățarea supervizată, fiind un model matematic al unui neuron biologic. În particular, _perceptronii multistrat (MLP)_ formează rețele neuronale cu mai multe straturi de perceptroni: un strat de intrare, unul sau mai multe straturi intermediare (ascunse), și un strat de ieșire.\n",
    "\n",
    "<div style=\"text-align:center\"><img style=\"width: 500px\" src=\"./Images/mlp_diagram.png\"><br>Un perceptron multistrat ilustrat.<br>sursă: <a href=\"https://github.com/ledell/sldm4-h2o/blob/master/sldm4_h2o_oct2016.pdf\">GitHub (ledell/sldm4-h2o)</a></div>\n",
    "\n",
    "<br>Într-o rețea neuronală, o _funcție de activare_ definește ieșirea unui perceptron după ce este supus unui set de intrare. În forma lui cea mai simplă, funcția poate returna un rezultat binar (funcție liniară, output 0 sau 1): făcând analogie cu neuronul biologic, dacă trece un impuls electric prin axonul acestuia sau nu. În cazul rețelelor neuronale moderne care utilizează mai multe straturi de perceptroni, funcțiile de activare pot fi și non-binare (non-liniare). Scikit-learn admite funcții de activare de ambele tipuri în implementarea MLP classifier:\n",
    "- _funcția identitate_: $f(x) = x$\n",
    "- _sigmoida logistică_: $f(x) = \\frac{1}{1 + \\exp(-x)}$\n",
    "- _tangenta hiperbolică_: $f(x) = \\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- _Rectified Linear Unit (ReLU)_: $f(x) = \\max(0, x) = \\begin{cases} 0 & \\text{dacă } x \\leq 0 \\\\ x & \\text{dacă } x > 0 \\end{cases}$\n",
    "\n",
    "De asemenea, clasificatorul MLP din Scikit-learn utilizează și algoritmi de optimizare a ponderilor (solvers): _LBFGS_ (algoritm Quasi-Newton), _SGD_ (stochastic gradient descent) și _Adam_ (algoritm derivat din SGD, creat de Diederik P. Kingma și Jimmy Lei Ba)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4. Gaussian Naïve Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.naive_bayes.GaussianNB`<i>(priors=None, var_smoothing=1e-09)<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmul de clasificare _Gaussian Naïve Bayes_ aparține familiei de clasificatori _Naïve Bayes_, care presupun că prezența unui feature într-o clasă nu este afectată de prezența altor features; pe scurt, proprietățile contribuie independent la probabilitatea apartenenței la o clasă. În particular, algoritmul _Gaussian Naïve Bayes_ urmează funcția de probabilitate (PDF) a unei distribuții normale (Gaussiene):\n",
    "$$\\large P(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y^2}}\\exp\\bigg(-\\frac{(x_i - \\mu_y)^2}{2 \\sigma_y^2}\\bigg),$$\n",
    "unde parametrii $\\sigma_y$ și $\\mu_y$, deviația standard și media, sunt determinați folosind maximum likelihood estimation (MLE), o metodă de estimare a parametrilor unei PDF prin maximizarea unei funcții de likelihood (cât de bine se potrivește un sample cu un model statistic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5. Random Forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.ensemble.RandomForestClassifier`<i>(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un clasificator _Random forest_ se folosește de ipotezele emise de mai mulți arbori de decizie aleatori (_random trees_), obținuți în urma unui _random split_. Un random forest se obține prin construirea unui random tree pentru fiecare set de antrenare. Acești arbori funcționează ca un ansamblu; pentru fiecare dată de intrare se aplică modelele din ansamblu, și rezultatul final se obține agregând rezultatele prin votare. Astfel, un random forest este un _meta-estimator_: se obține o predicție în urma mai multor predicții.\n",
    "<div style=\"text-align:center\"><img style=\"width: 400px\" src=\"./Images/rf_diagram.png\"><br>Un model random forest făcând o predicție; în urma votării se obține rezultatul 1.<br>sursă: <a href=\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\">Medium (Towards Data Science: Understanding Random Forest)</a></div>\n",
    "\n",
    "<br>La fel ca la _Decision Tree classifier_, pentru măsurarea calității unui split, Scikit-learn utilizează două metrici:\n",
    "\n",
    "- _impuritatea Gini_: $Gini(p) = 1 - \\sum_{j=1}^c p_j^2$ <br>\n",
    "- _entropia_: $H(p) = - \\sum_{j=1}^c p_j \\log p_j$\n",
    "\n",
    "(unde c este numărul de clase (etichete), iar $p_j$ este subsetul etichetat cu clasă i, unde $j \\in \\{1, 2, ..., c\\}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Testarea algoritmilor de clasificare pe setul de date cu Scikit-learn</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a pretty printing function, don't mind me...\n",
    "def print_stats_cv(model_cv_stats):\n",
    "    print(f\"Test accuracy for each fold: {model_cv_stats['test_accuracy']} \\n=> Average test accuracy: {round(model_cv_stats['test_accuracy'].mean() * 100, 3)}%\")\n",
    "    print(f\"Train accuracy for each fold: {model_cv_stats['train_accuracy']} \\n=> Average train accuracy: {round(model_cv_stats['train_accuracy'].mean() * 100, 3)}%\")\n",
    "    print(f\"Test F1 score for each fold: {model_cv_stats['test_f1_macro']} \\n=> Average test F1 score: {round(model_cv_stats['test_f1_macro'].mean() * 100, 3)}%\")\n",
    "    print(f\"Train F1 score for each fold: {model_cv_stats['train_f1_macro']} \\n=> Average train F1 score: {round(model_cv_stats['train_f1_macro'].mean() * 100, 3)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <i>k</i>-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>5-fold cross validation for 4-nearest neighbors classification:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for each fold: [0.9675 0.985  0.9675 0.9875 0.9775] \n",
      "=> Average test accuracy: 97.7%\n",
      "Train accuracy for each fold: [0.99     0.99     0.9925   0.99     0.990625] \n",
      "=> Average train accuracy: 99.062%\n",
      "Test F1 score for each fold: [0.96747175 0.98498649 0.96707199 0.98754412 0.97761589] \n",
      "=> Average test F1 score: 97.694%\n",
      "Train F1 score for each fold: [0.98999996 0.99002075 0.99250155 0.9900046  0.99063789] \n",
      "=> Average train F1 score: 99.063%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# hiperparametri\n",
    "knn_neighbors = 4\n",
    "knn_minkowski_p = 3\n",
    "\n",
    "# scalare date\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# implementare KNN\n",
    "model = KNeighborsClassifier(n_neighbors=knn_neighbors, p=knn_minkowski_p)\n",
    "model_cv_stats = cross_validate(model, X_scaled, y, cv=folds, scoring=('accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "# statistici\n",
    "display(HTML(f\"<h4>{folds}-fold cross validation for {knn_neighbors}-nearest neighbors classification:</h4>\"))\n",
    "print_stats_cv(model_cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>5-fold cross validation for Decision Trees classification:</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for each fold: [0.96   0.9325 0.9275 0.9825 0.9725] \n",
      "=> Average test accuracy: 95.5%\n",
      "Train accuracy for each fold: [1. 1. 1. 1. 1.] \n",
      "=> Average train accuracy: 100.0%\n",
      "Test F1 score for each fold: [0.95972496 0.93246313 0.92562893 0.98251804 0.97253289] \n",
      "=> Average test F1 score: 95.457%\n",
      "Train F1 score for each fold: [1. 1. 1. 1. 1.] \n",
      "=> Average train F1 score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# hiperparametri\n",
    "dt_criterion = 'gini'\n",
    "dt_splitter = 'best'\n",
    "\n",
    "# implementare Decision Tree\n",
    "model = DecisionTreeClassifier(criterion=dt_criterion, splitter=dt_splitter, random_state=42)\n",
    "model_cv_stats = cross_validate(model, X, y, cv=folds, scoring=('accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "# statistici\n",
    "display(HTML(f\"<h4>{folds}-fold cross validation for Decision Trees classification:</h4>\"))\n",
    "print_stats_cv(model_cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multilayer Perceptron (MLP) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>5-fold cross validation for MLP classification</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "using hardcoded hyperparameters - Solver: <b>adam</b>, Activation function: <b>relu</b>, Parameter for regularization (α): <b>0.001</b>, Hidden layer sizes: <b>(50, 50)</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for each fold: [0.9725 0.9975 0.95   0.975  0.9775] \n",
      "=> Average test accuracy: 97.45%\n",
      "Train accuracy for each fold: [0.98125  0.971875 0.983125 0.98125  0.981875] \n",
      "=> Average train accuracy: 97.988%\n",
      "Test F1 score for each fold: [0.97244414 0.99749994 0.94983905 0.97514624 0.97746665] \n",
      "=> Average test F1 score: 97.448%\n",
      "Train F1 score for each fold: [0.98124085 0.97171261 0.98314044 0.98122406 0.9819253 ] \n",
      "=> Average train F1 score: 97.985%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# hiperparametri\n",
    "mlp_solver = 'adam'\n",
    "mlp_activation = 'relu'\n",
    "mlp_alpha=1e-3\n",
    "mlp_hidden_layer_sizes = (50,50)\n",
    "\n",
    "# implementare MLP\n",
    "model = MLPClassifier(solver=mlp_solver, activation=mlp_activation, alpha=mlp_alpha, hidden_layer_sizes=mlp_hidden_layer_sizes, random_state=42)\n",
    "model_cv_stats = cross_validate(model, X, y, cv=folds, scoring=('accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "# statistici\n",
    "display(HTML(f\"<h4>{folds}-fold cross validation for MLP classification</h4>\"))\n",
    "display(HTML(f\"using hardcoded hyperparameters - Solver: <b>{mlp_solver}</b>, Activation function: <b>{mlp_activation}</b>, Parameter for regularization (α): <b>{mlp_alpha}</b>, Hidden layer sizes: <b>{mlp_hidden_layer_sizes}</b>\"))\n",
    "print_stats_cv(model_cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gaussian Naïve Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>5-fold cross validation for Gaussian NB classification</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for each fold: [0.99   0.9725 0.98   0.98   0.985 ] \n",
      "=> Average test accuracy: 98.15%\n",
      "Train accuracy for each fold: [0.98125  0.983125 0.9875   0.985    0.983125] \n",
      "=> Average train accuracy: 98.4%\n",
      "Test F1 score for each fold: [0.99001188 0.97255265 0.9799862  0.98006098 0.9849985 ] \n",
      "=> Average test F1 score: 98.152%\n",
      "Train F1 score for each fold: [0.98127765 0.98313802 0.98751983 0.98501642 0.9831722 ] \n",
      "=> Average train F1 score: 98.402%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# implementare GNB\n",
    "model = GaussianNB()\n",
    "model_cv_stats = cross_validate(model, X, y, cv=folds, scoring=('accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "# statistici\n",
    "display(HTML(f\"<h4>{folds}-fold cross validation for Gaussian NB classification</h4>\"))\n",
    "print_stats_cv(model_cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>5-fold cross validation for Random Forest classification</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for each fold: [0.9825 0.96   0.975  0.985  0.9875] \n",
      "=> Average test accuracy: 97.8%\n",
      "Train accuracy for each fold: [1. 1. 1. 1. 1.] \n",
      "=> Average train accuracy: 100.0%\n",
      "Test F1 score for each fold: [0.98244899 0.95974235 0.97494678 0.9850456  0.98749969] \n",
      "=> Average test F1 score: 97.794%\n",
      "Train F1 score for each fold: [1. 1. 1. 1. 1.] \n",
      "=> Average train F1 score: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# hiperparametri\n",
    "rfc_n_estimators = 150\n",
    "rfc_criterion = 'gini'\n",
    "\n",
    "# implementare Random Forest\n",
    "model = RandomForestClassifier(n_estimators=rfc_n_estimators, criterion=rfc_criterion)\n",
    "model_cv_stats = cross_validate(model, X, y, cv=folds, scoring=('accuracy', 'f1_macro'), return_train_score=True)\n",
    "\n",
    "display(HTML(f\"<h4>{folds}-fold cross validation for Random Forest classification</h4>\"))\n",
    "print_stats_cv(model_cv_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Nested Cross Validation pentru optimizarea hiperparametrilor</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation: split overview\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train row indices</th>\n",
       "      <th>Test row indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>[3, 7, 25, 29, 34, 46, 56, 63, 64, 65, 67, 69,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>[4, 5, 16, 40, 44, 51, 52, 55, 59, 73, 77, 87,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15...</td>\n",
       "      <td>[1, 13, 18, 19, 22, 27, 28, 32, 35, 36, 39, 41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 3, 4, 5, 7, 8, 13, 14, 16, 18, 19, 21, 22,...</td>\n",
       "      <td>[0, 2, 6, 9, 10, 11, 12, 15, 17, 20, 23, 24, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15...</td>\n",
       "      <td>[8, 14, 21, 26, 30, 33, 37, 45, 50, 60, 61, 62...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Fold                                  Train row indices  \\\n",
       "0    1  [0, 1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
       "1    2  [0, 1, 2, 3, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
       "2    3  [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15...   \n",
       "3    4  [1, 3, 4, 5, 7, 8, 13, 14, 16, 18, 19, 21, 22,...   \n",
       "4    5  [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15...   \n",
       "\n",
       "                                    Test row indices  \n",
       "0  [3, 7, 25, 29, 34, 46, 56, 63, 64, 65, 67, 69,...  \n",
       "1  [4, 5, 16, 40, 44, 51, 52, 55, 59, 73, 77, 87,...  \n",
       "2  [1, 13, 18, 19, 22, 27, 28, 32, 35, 36, 39, 41...  \n",
       "3  [0, 2, 6, 9, 10, 11, 12, 15, 17, 20, 23, 24, 3...  \n",
       "4  [8, 14, 21, 26, 30, 33, 37, 45, 50, 60, 61, 62...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# CVs configuration\n",
    "inner_cv = KFold(n_splits=4, shuffle=True)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# outer CV folds:\n",
    "print(\"5-fold cross validation: split overview\")\n",
    "splits = outer_cv.split(range(data_wifi.index.size))\n",
    "subsets = pd.DataFrame(columns=['Fold', 'Train row indices', 'Test row indices'])\n",
    "for i, split_data in enumerate(splits):\n",
    "    subsets.loc[i]=[i + 1, split_data[0], split_data[1]]\n",
    "display(subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <i>k</i>-nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 1, optimal hyperparameters after inner 4-fold CV: {'p': 3, 'n_neighbors': 7}\n",
      "kNN model accuracy with optimal hyperparameters: 0.9775\n",
      "Outer fold 2, optimal hyperparameters after inner 4-fold CV: {'p': 3, 'n_neighbors': 3}\n",
      "kNN model accuracy with optimal hyperparameters: 0.9775\n",
      "Outer fold 3, optimal hyperparameters after inner 4-fold CV: {'p': 3, 'n_neighbors': 3}\n",
      "kNN model accuracy with optimal hyperparameters: 0.9925\n",
      "Outer fold 4, optimal hyperparameters after inner 4-fold CV: {'p': 3, 'n_neighbors': 3}\n",
      "kNN model accuracy with optimal hyperparameters: 0.9925\n",
      "Outer fold 5, optimal hyperparameters after inner 4-fold CV: {'p': 3, 'n_neighbors': 3}\n",
      "kNN model accuracy with optimal hyperparameters: 0.98\n",
      "\n",
      "Average model accuracy: 98.4%\n"
     ]
    }
   ],
   "source": [
    "outer_cv_acc = []\n",
    "param_candidates = {'n_neighbors': np.linspace(start=1, stop=30, num=30, dtype=int),\n",
    "                    'p': np.linspace(start=1, stop=5, num=4, dtype=int)} \n",
    "param_search = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions=param_candidates, scoring='accuracy', cv=inner_cv, random_state=42)\n",
    "    \n",
    "for fold in range(5):\n",
    "    X_train = X_scaled[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    y_train = y[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    X_test = X_scaled[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    y_test = y[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    \n",
    "    param_search.fit(X_train, y_train)\n",
    "    y_estimated = param_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_estimated)\n",
    "    outer_cv_acc.append(accuracy)\n",
    "    \n",
    "    print(f'Outer fold {fold+1}, optimal hyperparameters after inner 4-fold CV: {param_search.best_params_}')\n",
    "    print(f'kNN model accuracy with optimal hyperparameters: {accuracy}')\n",
    "    \n",
    "print(f'\\nAverage model accuracy: {round(np.mean(outer_cv_acc) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 1, optimal hyperparameters after inner 4-fold CV: {'criterion': 'entropy', 'splitter': 'best'}\n",
      "Decision Tree model accuracy with optimal hyperparameters: 0.965\n",
      "Outer fold 2, optimal hyperparameters after inner 4-fold CV: {'criterion': 'gini', 'splitter': 'best'}\n",
      "Decision Tree model accuracy with optimal hyperparameters: 0.9675\n",
      "Outer fold 3, optimal hyperparameters after inner 4-fold CV: {'criterion': 'gini', 'splitter': 'best'}\n",
      "Decision Tree model accuracy with optimal hyperparameters: 0.97\n",
      "Outer fold 4, optimal hyperparameters after inner 4-fold CV: {'criterion': 'entropy', 'splitter': 'best'}\n",
      "Decision Tree model accuracy with optimal hyperparameters: 0.9825\n",
      "Outer fold 5, optimal hyperparameters after inner 4-fold CV: {'criterion': 'gini', 'splitter': 'best'}\n",
      "Decision Tree model accuracy with optimal hyperparameters: 0.9525\n",
      "\n",
      "Average model accuracy: 96.75%\n"
     ]
    }
   ],
   "source": [
    "outer_cv_acc = []\n",
    "param_candidates = {'criterion': ['gini', 'entropy'],\n",
    "                    'splitter': ['best', 'random']} \n",
    "param_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_candidates, scoring='accuracy', cv=inner_cv)\n",
    "\n",
    "for fold in range(5):\n",
    "    X_train = X[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    y_train = y[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    X_test = X[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    y_test = y[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    \n",
    "    param_search.fit(X_train, y_train)\n",
    "    y_estimated = param_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_estimated)\n",
    "    outer_cv_acc.append(accuracy)\n",
    "    \n",
    "    print(f'Outer fold {fold+1}, optimal hyperparameters after inner 4-fold CV: {param_search.best_params_}')\n",
    "    print(f'Decision Tree model accuracy with optimal hyperparameters: {accuracy}')\n",
    "    \n",
    "print(f'\\nAverage model accuracy: {round(np.mean(outer_cv_acc) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multilayer Perceptron (MLP) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 1, optimal hyperparameters after inner 4-fold CV: {'alpha': 0.0468937875751503, 'activation': 'logistic'}\n",
      "MLP model accuracy with optimal hyperparameters: 0.9825\n",
      "Outer fold 2, optimal hyperparameters after inner 4-fold CV: {'alpha': 0.04809619238476954, 'activation': 'relu'}\n",
      "MLP model accuracy with optimal hyperparameters: 0.97\n",
      "Outer fold 3, optimal hyperparameters after inner 4-fold CV: {'alpha': 0.04609218436873747, 'activation': 'relu'}\n",
      "MLP model accuracy with optimal hyperparameters: 0.98\n",
      "Outer fold 4, optimal hyperparameters after inner 4-fold CV: {'alpha': 0.1, 'activation': 'logistic'}\n",
      "MLP model accuracy with optimal hyperparameters: 0.9775\n",
      "Outer fold 5, optimal hyperparameters after inner 4-fold CV: {'alpha': 0.002004008016032064, 'activation': 'tanh'}\n",
      "MLP model accuracy with optimal hyperparameters: 0.9625\n",
      "\n",
      "Average model accuracy: 97.45%\n"
     ]
    }
   ],
   "source": [
    "outer_cv_acc = []\n",
    "param_candidates = {'alpha': np.linspace(start=0, stop=1e-1, num=500),\n",
    "                    'activation': ['identity', 'logistic', 'tanh', 'relu']}\n",
    "param_search = RandomizedSearchCV(estimator=MLPClassifier(max_iter=1000), param_distributions=param_candidates, scoring='accuracy', cv=inner_cv)\n",
    "\n",
    "for fold in range(5):\n",
    "    X_train = X[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    y_train = y[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    X_test = X[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    y_test = y[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    \n",
    "    param_search.fit(X_train, y_train)\n",
    "    y_estimated = param_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_estimated)\n",
    "    outer_cv_acc.append(accuracy)\n",
    "     \n",
    "    print(f'Outer fold {fold+1}, optimal hyperparameters after inner 4-fold CV: {param_search.best_params_}')\n",
    "    print(f'MLP model accuracy with optimal hyperparameters: {accuracy}')\n",
    "     \n",
    "print(f'\\nAverage model accuracy: {round(np.mean(outer_cv_acc) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gaussian Naïve Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 1, optimal hyperparameters after inner 4-fold CV: {'var_smoothing': 0.0072545092925851715}\n",
      "Gaussian Naïve Bayes model accuracy with optimal hyperparameters: 0.98\n",
      "Outer fold 2, optimal hyperparameters after inner 4-fold CV: {'var_smoothing': 0.0027054115511022047}\n",
      "Gaussian Naïve Bayes model accuracy with optimal hyperparameters: 0.9825\n",
      "Outer fold 3, optimal hyperparameters after inner 4-fold CV: {'var_smoothing': 1e-09}\n",
      "Gaussian Naïve Bayes model accuracy with optimal hyperparameters: 0.9775\n",
      "Outer fold 4, optimal hyperparameters after inner 4-fold CV: {'var_smoothing': 0.007875751715430862}\n",
      "Gaussian Naïve Bayes model accuracy with optimal hyperparameters: 0.99\n",
      "Outer fold 5, optimal hyperparameters after inner 4-fold CV: {'var_smoothing': 0.008897795701402806}\n",
      "Gaussian Naïve Bayes model accuracy with optimal hyperparameters: 0.985\n",
      "\n",
      "Average model accuracy: 98.3%\n"
     ]
    }
   ],
   "source": [
    "outer_cv_acc = []\n",
    "param_candidates = {'var_smoothing': np.linspace(start=1e-9, stop=1e-2, num=500)} \n",
    "param_search = RandomizedSearchCV(estimator=GaussianNB(), param_distributions=param_candidates, scoring='accuracy', cv=inner_cv)\n",
    "\n",
    "for fold in range(5):\n",
    "    X_train = X[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    y_train = y[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    X_test = X[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    y_test = y[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    \n",
    "    param_search.fit(X_train, y_train)\n",
    "    y_estimated = param_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_estimated)\n",
    "    outer_cv_acc.append(accuracy)\n",
    "    \n",
    "    print(f'Outer fold {fold+1}, optimal hyperparameters after inner 4-fold CV: {param_search.best_params_}')\n",
    "    print(f'Gaussian Naïve Bayes model accuracy with optimal hyperparameters: {accuracy}')\n",
    "    \n",
    "print(f'\\nAverage model accuracy: {round(np.mean(outer_cv_acc) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 1, optimal hyperparameters after inner 4-fold CV: {'n_estimators': 344, 'criterion': 'entropy'}\n",
      "Random Forest Classifier model accuracy with optimal hyperparameters: 0.9875\n",
      "Outer fold 2, optimal hyperparameters after inner 4-fold CV: {'n_estimators': 138, 'criterion': 'entropy'}\n",
      "Random Forest Classifier model accuracy with optimal hyperparameters: 0.9825\n",
      "Outer fold 3, optimal hyperparameters after inner 4-fold CV: {'n_estimators': 239, 'criterion': 'entropy'}\n",
      "Random Forest Classifier model accuracy with optimal hyperparameters: 0.98\n",
      "Outer fold 4, optimal hyperparameters after inner 4-fold CV: {'n_estimators': 336, 'criterion': 'gini'}\n",
      "Random Forest Classifier model accuracy with optimal hyperparameters: 0.98\n",
      "Outer fold 5, optimal hyperparameters after inner 4-fold CV: {'n_estimators': 15, 'criterion': 'gini'}\n",
      "Random Forest Classifier model accuracy with optimal hyperparameters: 0.9775\n",
      "\n",
      "Average model accuracy: 98.15%\n"
     ]
    }
   ],
   "source": [
    "outer_cv_acc = []\n",
    "param_candidates = {'criterion': ['gini', 'entropy'],\n",
    "                   'n_estimators': np.linspace(start=1, stop=500, num=500, dtype=int)} \n",
    "param_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_candidates, scoring='accuracy', cv=inner_cv)\n",
    "\n",
    "for fold in range(5):\n",
    "    X_train = X[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    y_train = y[subsets.loc[subsets.index[fold],'Train row indices']]\n",
    "    X_test = X[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    y_test = y[subsets.loc[subsets.index[fold],'Test row indices']]\n",
    "    \n",
    "    param_search.fit(X_train, y_train)\n",
    "    y_estimated = param_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_estimated)\n",
    "    outer_cv_acc.append(accuracy)\n",
    "    \n",
    "    print(f'Outer fold {fold+1}, optimal hyperparameters after inner 4-fold CV: {param_search.best_params_}')\n",
    "    print(f'Random Forest Classifier model accuracy with optimal hyperparameters: {accuracy}')\n",
    "    \n",
    "print(f'\\nAverage model accuracy: {round(np.mean(outer_cv_acc) * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
