{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentație modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.linear_model.Lasso`_(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriere\n",
    "\n",
    "Estimatorul Lasso<sup>[[1]](https://www.youtube.com/watch?v=NGf0voTMlcs)[[2]](https://scikit-learn.org/stable/modules/linear_model.html#lasso)[[3]](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso)</sup> funcționează cel mai bine atunci când setul de date conține multe features inutile, acestea din urmă fiind pur și simplu eliminate, simplificând funcția obiectiv. La fel ca și estimatorul Ridge, produce un model care nu prezice datele de antrenare perfect, ci i se adaugă un bias pentru a face predicții mai bune. Funcția obiectiv pe care Lasso încearcă să o minimizeze este:\n",
    "\n",
    "$$\\min_w \\frac{1}{2 \\cdot \\text{number of samples}} \\cdot \\left\\lVert y - Xw \\right\\rVert^2_2 + \\alpha \\cdot \\left\\lVert w \\right \\rVert_1$$\n",
    "\n",
    "Cu alte cuvinte, funcția obiectiv este $(\\text{suma pătratelor reziduurilor} + \\alpha \\cdot |\\text{panta}|)$, comparativ cu Ridge Regression, sintetizată astfel: $(\\text{suma pătratelor reziduurilor} + \\alpha \\cdot \\text{panta}^2)$. De aici putem deduce că panta modelului obținut de Lasso poate ajunge la 0 pe măsură ce parametrul $\\alpha$ crește, în contrast cu Ridge, unde devine asimptotic zero.\n",
    "\n",
    "<div style = \"text-align:center\"> <img width=\"350px\" height=\"350px\" src = \"./Images/Lasso Regression slope zero.png\" /> <br> Panta dreptei devine zero pentru $\\alpha = 10000$ (portocaliu). <br>Panta în cazul $\\alpha = 0$ (roșu) echivalează modelul cu unul OLS.</div>\n",
    "\n",
    "### Hiperparametri\n",
    "- `alpha`: constanta care multiplică termenul de regularizare L1, implicit 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [Bayesian Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.linear_model.BayesianRidge`_(*, n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriere\n",
    "\n",
    "Bayesian Ridge Regression<sup>[[1]](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression)</sup> este un algoritm de regresie similar cu modelul clasic Ridge, dar estimează un model probabilistic. O regresie Bayesiană are avantajul de a se adapta ușor la datele existente, dar dezavantajul ei este estimarea modelului după o perioadă lungă de timp. Ponderile obținute în urma unei regresii Bayesian Ridge diferă puțin față de cele obținute de OLS, dar este considerat mai robust. Modelul probabilistic se folosește de următoarea formulă a probabilității _a priori_ a ponderilor $w$ în procesul de inferență, dându-se $\\lambda$: \n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0, \\lambda^{-1} \\textbf{I}_p) \\text{  (distribuție Gaussiană în jurul lui 0, cu dispersie }\\lambda^{-1} \\textbf{I}_p \\text{) }$$\n",
    "\n",
    "Ridge (regularizarea Tikhonov)<sup>[[2]](https://en.wikipedia.org/wiki/Tikhonov_regularization)</sup>, estimatorul clasic care stă la baza regresiei Bayesian Ridge, caută un model care nu estimează datele de antrenare la perfecție (deci fără overfit), în schimb, se aplică intenționat un bias<sup>[[3]](https://youtu.be/Q81RR3yKn30?t=244)</sup> modelului care va determina scăderea erorii pentru setul de testare.\n",
    "\n",
    "<div style = \"text-align:center\"> <img width=\"350px\" height=\"350px\" src = \"./Images/Ridge Regression in a nutshell.png\" /> <br> Modul de funcționare al estimatorului Ridge.</div><br>\n",
    "\n",
    "### Hiperparametri\n",
    "- `alpha_1`: parametrul de shape al distribuției Gamma a probabilității a priori$^*$ a lui $\\alpha$\n",
    "- `alpha_2`: parametrul de rate al distribuției Gamma a probabilității a priori a lui $\\alpha$\n",
    "- `lambda_1`: parametrul de shape al distribuției Gamma a probabilității a priori a lui $\\lambda$\n",
    "- `lambda_2`: parametrul de rate al distribuției Gamma a probabilității a priori a lui $\\lambda$\n",
    "\n",
    "$^*$Probabilitatea unei ipoteze $h$, $p(h)$, conform unui model probabilistic care ia în considerare toată informația primară referitoare la $h$ se numește _prior_ probability (probabilitate _a priori_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.svm.SVR`_(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriere\n",
    "\n",
    "În 1996<sup>[[1]](http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf)[[2]](https://en.wikipedia.org/wiki/Support_vector_machine#Regression)</sup>, Harris Drucker, Christ Burges, Linda Kaufman, Alexander Smola și Vladimir Vapnik au propus o versiune a modelului Support-vector machines (SVM) pentru regresie, numită Support-vector regression (SVR). Comparativ cu ceilalți algoritmi de regresie, modelul produs de SVR depinde doar de un subset al setului de antrenare, deoarece funcția de eroare ignoră datele care sunt foarte apropape de predicția modelului. \n",
    "\n",
    "`SVR` din `sklearn` este un Epsilon-Support Vector Regression (ε-SVR)<sup>[[3]](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR)</sup>, deci epsilon este unul dintre hiperparametri. `sklearn.svm.SVR` este recomandat pentru seturi mici de date, durata de fit crescând pentru seturi mai mari de 10000 de trăsături.\n",
    "\n",
    "Scopul lui ε-SVR <sup>[[4]](https://books.google.ro/books?id=72C-DwAAQBAJ&pg=PA126&lpg=PA126&dq=%22ε-SVR%22&source=bl&ots=3TJ-FdqxA9&sig=ACfU3U3WCVV0Efj6A98-OsAr99kJ_vFOFA&hl=en&sa=X&ved=2ahUKEwjBgfqmlbbpAhUVHHcKHfHnDQkQ6AEwD3oECAYQAQ#v=onepage&q=%22ε-SVR%22&f=false)</sup> este de a estima o funcție, fiind constrâns ca estimarea de către funcție a fiecărei intrări să aibă o deviație de cel mult ε față de intrare, deci va penaliza predicțiile care sunt mai depărtate de ε decât rezultatul așteptat. În cazul unei funcții liniare $f$, aceasta va avea forma:\n",
    "\n",
    "$$\\large y = w^\\top X + b$$\n",
    "\n",
    "ε-SVR găsește o aproximație a funcției $f$ prin identificarea unui \"tub insensibil la epsilon cât mai plat\", cu alte cuvinte, caută ponderi cât mai mici. În final, funcția care va aproxima $f $va avea următoarea formă: \n",
    "\n",
    "$$\\large \\min_w \\frac{1}{2} \\left \\lVert w \\right \\rVert^2, \\text{unde} \n",
    "     \\begin{cases}\n",
    "       y_i - w^\\top X_i - b \\leq \\epsilon \\\\\n",
    "       w^\\top X_i + b - y_i \\leq \\epsilon \\\\ \n",
    "     \\end{cases}$$\n",
    "     \n",
    "<div style = \"text-align:center\"> <img height=\"500px\" src = \"./Images/SVR kernel trick.png\" /> <br> ε-SVR nonliniar cu date liniar separabile. Datele de intrare inițial neseparabile liniar <br>au fost translatate printr-o funcție kernel φ și separate printr-un hiperplan liniar.</div><br>\n",
    "\n",
    "### Hiperparametri\n",
    "- `kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}`: funcția kernel utilizată de SVR pentru translatarea datelor\n",
    "- `C`: coeficient de regularizare, strict pozitiv\n",
    "- `epsilon`: parametrul ε care influențează identificarea ponderilor în ε-SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.linear_model.ElasticNet`_(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriere\n",
    "\n",
    "În anul 2005, Hui Zou și Trevor Hastie au creat modelul de regresie ElasticNet<sup>[[1]](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696)</sup> pentru a-l îmbunătăți pe cel de Lasso. Estimatorul Lasso funcționează cel mai bine atunci când setul de date conține multe trăsături inutile, acestea din urmă fiind eliminate, simplificând funcția obiectiv. În schimb, estimatorul Ridge este deseori utilizat atunci când majoritatea trăsăturilor sunt utile. În cazul unor seturi de date cu foarte multe trăsături, alegerea unui estimator poate fi o problemă, deoarece nu putem cunoaște toate trăsăturile și nu le putem estima importanța. ElasticNet rezolvă problema, oferind regularizare flexibilă <sup>[[4]](https://youtu.be/1dKRdX9bfIo?t=46)</sup>  (`l1_ratio` în `sklearn`) și profitând de avantajele fiecărui regresor. \n",
    "\n",
    "ElasticNet extinde Lasso adăugându-se încă un termen de penalizare L2 funcției obiectiv, combinând astfel penalizările L1 și L2 ale regresorilor Lasso și Ridge. Practic, Scikit-learn minimizează următoarea funcție<sup>[[2]](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet)</sup><sup>[[3]](https://en.wikipedia.org/wiki/Lasso_(statistics)#Elastic_net)</sup>:\n",
    "\n",
    "$$\\large \\frac{1}{2 \\cdot \\text{number of samples}} \\cdot \\left\\lVert y - Xw \\right\\rVert^2_2 + \\alpha \\cdot \\text{L1 ratio} \\cdot \\left\\lVert w \\right \\rVert_1 + 0.5 \\cdot \\alpha \\cdot (1 - \\text{L1 ratio}) \\cdot \\left\\lVert w \\right \\rVert^2_2,$$\n",
    "\n",
    "cu $\\text{l1_ratio}$ implicit $0.5$ și $\\alpha = 1$. Cu alte cuvinte<sup>[[3]](https://en.wikipedia.org/wiki/Lasso_(statistics)#Elastic_net)</sup><sup>[[4]](https://youtu.be/1dKRdX9bfIo?t=170)</sup>, funcția obiectiv minimizată de ponderile $\\beta$ este: \n",
    "\n",
    "$$\\text{Least Squares (SSR) } \\left \\lVert y - X\\beta \\right \\rVert_2^2 + \\text{Lasso Regr. Penalty } \\lambda_1 \\left \\lVert \\beta \\right \\rVert_1 + \\text{Ridge Regr. Penalty (Tikhonov reg.) } \\lambda_2 \\left \\lVert \\beta \\right \\rVert_2^2.$$\n",
    "\n",
    "<div style = \"text-align:center\"> <img width=\"600px\" src = \"./Images/Ridge vs. Lasso.png\" /> <br> Regresorul ElasticNet se bucură de avantajele a doi estimatori simultan: Ridge și Lasso.</div><br>\n",
    "\n",
    "### Hiperparametri\n",
    "- `alpha` este o constantă echivalentă cu OLS, estimatorul utilizat în regresia liniară.\n",
    "- `l1_ratio` definește raportul dintre impactul fiecărui termen de penalizare asupra funcției obiectiv. Pentru `0` se utilizează exclusiv penalizarea L2; pentru `1` exclusiv penalizarea L1, iar pentru valori în `[0-1]` penalizarea este o combinație între L1 și L2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Decision Tree Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class `sklearn.tree.DecisionTreeRegressor`_(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriere\n",
    "\n",
    "Decision Trees<sup>[[1]](https://scikit-learn.org/stable/modules/tree.html#tree)[[2]](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor)</sup> (arborii de decizie) sunt o metodă de învățare supervizată utilizată atât pentru clasificare, cât și pentru regresie. Un arbore de decizie este o structură arborescentă tip flowchart unde un nod intern reprezintă un feature, ramura este un criteriu de decizie, iar fiecare frunză este un rezultat; în cazul regresiei, o predicție. Cu cât e mai adânc arborele, cu atât este mai precis arborele și sunt regulile de decizie mai complexe.\n",
    "\n",
    "Scopul unui Decision Tree Regressor este de a crea un model care să facă predicții precise, deducând reguli simple de decizie din trăsăturile datelor de intrare. Arborii de decizie au avantajele de a fi ușor de utilizat și vizualizat, nu necesită normalizarea datelor, au performanță bună chiar dacă presupunerile făcute nu respectă întocmai modelul adevărat obținut din datele de intrare, dar riscă să genereze arbori mult prea complecși care nu ilustrează bine datele de testare. De aceea se recomandă optimizarea hiperparametrilor, i.e. ajustarea adâncimii maxime pentru a preveni fenomenul de overfitting.\n",
    "\n",
    "<div style = \"text-align:center\"> <img width=\"500px\" src = \"./Images/Decision Tree sine curve.png\" /> <br> Doi arbori de decizie care aproximează o curbă sinusoidală, cu diferite adâncimi.</div><br>\n",
    "\n",
    "### Hiperparametri\n",
    "- `criterion: {“mse”, “friedman_mse”, “mae”}`: funcție care măsoară calitatea unui split.\n",
    "- `splitter: {“best”, “random”}`: strategia de split la fiecare nod\n",
    "- `max_depth`: adâncimea maximă a unui arbore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
